{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d6723b23f442d181cb1079969e3c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt1=695.0, cnt2=693.0\n",
      "cnt1=306.0, cnt2=305.0\n",
      "cnt1=504.0, cnt2=500.0\n",
      "cnt1=518.0, cnt2=474.0\n",
      "cnt1=563.0, cnt2=446.0\n",
      "cnt1=293.0, cnt2=292.0\n",
      "cnt1=450.0, cnt2=448.0\n",
      "cnt1=731.0, cnt2=728.0\n",
      "cnt1=990.0, cnt2=986.0\n",
      "cnt1=319.0, cnt2=318.0\n",
      "cnt1=763.0, cnt2=757.0\n",
      "cnt1=382.0, cnt2=380.0\n",
      "cnt1=612.0, cnt2=606.0\n",
      "cnt1=396.0, cnt2=391.0\n",
      "cnt1=461.0, cnt2=458.0\n",
      "cnt1=735.0, cnt2=718.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "ORIGINAL = r\"H:\\My Drive\\Megatron_DeepFake\\dataset\\original_sequences\\youtube\\raw\\videos\"\n",
    "PROCESSED = r\"H:\\My Drive\\Megatron_DeepFake\\dataset_processed\\original_sequences\\youtube\\raw\\videos\"\n",
    "# Collect video files\n",
    "VIDEO_FILES = []\n",
    "for root, _, files in os.walk(PROCESSED):\n",
    "    for file in files:\n",
    "        if file.endswith(\".mp4\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "            if os.path.exists(full_path):  # Check if the file exists\n",
    "                VIDEO_FILES.append(file)\n",
    "\n",
    "PAIRS = [(os.path.join(ORIGINAL,file), os.path.join(PROCESSED,file)) for file in VIDEO_FILES]\n",
    "\n",
    "for pair in tqdm(PAIRS, total = len(PAIRS)):\n",
    "    cap1 = cv2.VideoCapture(pair[0])\n",
    "    cap2 = cv2.VideoCapture(pair[1])\n",
    "    cnt1 = cap1.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    cnt2 = cap2.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    if cnt1!=cnt2:\n",
    "        print(f\"{cnt1=}, {cnt2=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd8ed7e07ca42319b62ba81625dcdc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt1=735.0, cnt2=710.0\n",
      "cnt1=413.0, cnt2=412.0\n",
      "cnt1=504.0, cnt2=503.0\n",
      "cnt1=350.0, cnt2=345.0\n",
      "cnt1=447.0, cnt2=444.0\n",
      "cnt1=504.0, cnt2=501.0\n",
      "cnt1=518.0, cnt2=474.0\n",
      "cnt1=390.0, cnt2=375.0\n",
      "cnt1=833.0, cnt2=714.0\n",
      "cnt1=450.0, cnt2=432.0\n",
      "cnt1=411.0, cnt2=321.0\n",
      "cnt1=990.0, cnt2=986.0\n",
      "cnt1=327.0, cnt2=319.0\n",
      "cnt1=348.0, cnt2=347.0\n",
      "cnt1=763.0, cnt2=754.0\n",
      "cnt1=484.0, cnt2=475.0\n",
      "cnt1=369.0, cnt2=170.0\n",
      "cnt1=328.0, cnt2=242.0\n",
      "cnt1=612.0, cnt2=606.0\n",
      "cnt1=809.0, cnt2=797.0\n",
      "cnt1=649.0, cnt2=629.0\n",
      "cnt1=396.0, cnt2=386.0\n",
      "cnt1=461.0, cnt2=453.0\n",
      "cnt1=334.0, cnt2=333.0\n",
      "cnt1=313.0, cnt2=296.0\n",
      "cnt1=698.0, cnt2=67.0\n",
      "cnt1=606.0, cnt2=604.0\n",
      "cnt1=315.0, cnt2=313.0\n",
      "cnt1=1249.0, cnt2=1220.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "ORIGINAL = r\"H:\\My Drive\\Megatron_DeepFake\\dataset\\manipulated_sequences\\Deepfakes\\raw\\videos\"\n",
    "PROCESSED = r\"H:\\My Drive\\Megatron_DeepFake\\dataset_processed\\manipulated_sequences\\Deepfakes\\raw\\videos\"\n",
    "# Collect video files\n",
    "VIDEO_FILES = []\n",
    "for root, _, files in os.walk(PROCESSED):\n",
    "    for file in files:\n",
    "        if file.endswith(\".mp4\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "            if os.path.exists(full_path):  # Check if the file exists\n",
    "                VIDEO_FILES.append(file)\n",
    "\n",
    "PAIRS = [(os.path.join(ORIGINAL,file), os.path.join(PROCESSED,file)) for file in VIDEO_FILES]\n",
    "\n",
    "for pair in tqdm(PAIRS, total = len(PAIRS)):\n",
    "    cap1 = cv2.VideoCapture(pair[0])\n",
    "    cap2 = cv2.VideoCapture(pair[1])\n",
    "    cnt1 = cap1.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    cnt2 = cap2.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    if cnt1!=cnt2:\n",
    "        print(f\"{cnt1=}, {cnt2=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9409, 0.3968],\n",
       "         [0.3900, 0.8180]],\n",
       "\n",
       "        [[0.0655, 0.4504],\n",
       "         [0.7174, 0.6440]],\n",
       "\n",
       "        [[0.9757, 0.6250],\n",
       "         [0.0649, 0.9505]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((3,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(t, idx):\n",
    "    if idx == 0:\n",
    "        return None, t\n",
    "    elif t.shape[0] > idx:\n",
    "        return t[:idx, :, :], t[idx:, :, :]\n",
    "    else:\n",
    "        return t[:, :, :], None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 7, 7]), torch.Size([3, 7, 7]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = split(torch.rand((5,7,7)),2)\n",
    "a.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([0, 7, 7]), torch.Size([5, 7, 7]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = split(torch.rand((5,7,7)),0)\n",
    "a.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.randint(0,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving batch as is torch.Size([128, 20, 384])\n",
      "After concatenation batch.shape=torch.Size([128, 20, 384])\n",
      "after the split, batch should be of size batch_size batch.shape=torch.Size([128, 20, 384]), while accumulator must have the excess if present None\n",
      "Saving batch as is torch.Size([128, 20, 384])\n",
      "torch.Size([128, 20, 384])\n",
      "torch.Size([128, 20, 384])\n",
      "torch.Size([128, 20, 384])\n"
     ]
    }
   ],
   "source": [
    "accumulator = None\n",
    "batch_sizes = [128,12,116,128,12,128]\n",
    "batch_size = 128\n",
    "r = range(4)\n",
    "result = []\n",
    "for i in r:\n",
    "    batch = torch.rand(batch_sizes[i], 20, 384)\n",
    "    \n",
    "    if accumulator is None:\n",
    "        if batch.shape[0] == batch_size:\n",
    "            print(f\"Saving batch as is {batch.shape}\")\n",
    "            result.append(batch)\n",
    "        else:\n",
    "            accumulator = batch\n",
    "    else:\n",
    "        if (accumulator.shape[0] + batch.shape[0]) < batch_size:\n",
    "            accumulator = torch.cat((accumulator, batch))\n",
    "        else:\n",
    "            batch = torch.cat((accumulator,batch))\n",
    "            print(f\"After concatenation {batch.shape=}\")\n",
    "            batch, accumulator = split(batch, batch_size)\n",
    "            print(f\"after the split, batch should be of size batch_size {batch.shape=}, while accumulator must have the excess if present {accumulator.shape if accumulator is not None else None}\")\n",
    "            result.append(batch)\n",
    "if accumulator is not None:\n",
    "    result.append(accumulator)\n",
    "\n",
    "for elem in result:\n",
    "    print(elem.shape)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.3440, 0.2073],\n",
       "          [0.6083, 0.1021]],\n",
       " \n",
       "         [[0.5546, 0.6392],\n",
       "          [0.7934, 0.6310]]]),)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(torch.rand((2,2,2)), 2,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _load_cached_file_paths(\n",
    "        self, dataloader, prefix, rgb_files, depth_files, label_files\n",
    "    ) -> int:\n",
    "        if len(os.listdir(self.tmp)) == 0:\n",
    "            return 0\n",
    "        for index in tqdm(\n",
    "            range(len(dataloader)),\n",
    "            total=len(dataloader),\n",
    "            desc=f\"Loading cached {prefix} data\",\n",
    "        ):\n",
    "            filenames = [\n",
    "                self.tmp / f\"{prefix}_rgb_batch_{index}\",\n",
    "                self.tmp / f\"{prefix}_depth_batch_{index}\",\n",
    "                self.tmp / f\"{prefix}_labels_batch_{index}\",\n",
    "            ]\n",
    "\n",
    "            if filenames[0].exists():\n",
    "                rgb_files.append(filenames[0])\n",
    "            if filenames[1].exists():\n",
    "                depth_files.append(filenames[1])\n",
    "            if filenames[2].exists():\n",
    "                label_files.append(filenames[2])\n",
    "        return min(len(rgb_files), len(depth_files), len(label_files))\n",
    "\n",
    "    def _cache_data(self, dataloader, prefix, rgb_files, depth_files, label_files):\n",
    "        \"\"\"\n",
    "        Cache data batches to disk.\n",
    "        \"\"\"\n",
    "        start_index = self._load_cached_file_paths(\n",
    "            dataloader, prefix, rgb_files, depth_files, label_files\n",
    "        )\n",
    "\n",
    "        for index, batch in tqdm(\n",
    "            enumerate(dataloader[start_index:], start=start_index),\n",
    "            initial=start_index,\n",
    "            total=len(dataloader) - start_index,\n",
    "            desc=f\"Caching {prefix} data\",\n",
    "        ):\n",
    "            filenames = [\n",
    "                self.tmp / f\"{prefix}_rgb_batch_{index}\",\n",
    "                self.tmp / f\"{prefix}_depth_batch_{index}\",\n",
    "                self.tmp / f\"{prefix}_labels_batch_{index}\",\n",
    "            ]\n",
    "            rgb_frames, depth_frames, labels = batch\n",
    "\n",
    "            for torch_data, filename in zip(\n",
    "                (rgb_frames, depth_frames, labels), filenames\n",
    "            ):\n",
    "                torch.save(torch_data, filename)\n",
    "            rgb_files.append(filenames[0])\n",
    "            depth_files.append(filenames[1])\n",
    "            label_files.append(filenames[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
